* NO_Diffusion

** Introduction

This is Chris Beckham's code to run diffusion operators on the Volcano dataset, to accompany the following paper:

#+begin_src
@article{lim2023score,
  title={Score-based diffusion models in function space},
  author={Lim, Jae Hyun and Kovachki, Nikola B and Baptista, Ricardo and Beckham, Christopher and Azizzadenesheli, Kamyar and Kossaifi, Jean and Voleti, Vikram and Song, Jiaming and Kreis, Karsten and Kautz, Jan and others},
  journal={arXiv preprint arXiv:2302.07400},
  year={2023}
}
#+end_src

** Installation

Requirements are:

- =torch= (>=1.12 should work)
- =torchvision= (>=0.13.1 should work)
- =omegaconf=
- =neuraloperator= (commit `8a0f5260983efffbe6e8ec39c2d095dba8cb2505`)
- =tqdm=

This repository was developed on a cluster that implements Slurm for scheduling and running experiments, and therefore it is built accommodating its various intricacies. However running jobs without it is also supported here.

Before we start running experiments we need to set up =env.sh=, which is in the =exps= directory. This file will /need to be modified/ to reflect /your/ desired environment, specifically one that has the aforementioned requirements installed (e.g. PyTorch, neural operators, etc.):

#+begin_src bash

# env.sh

# Load your virtualenv/conda env here that has the above
# requirements installed.
# ...

# Specify directory to save results to
export SAVEDIR=...
# Specify directory where volcano data is
export DATA_DIR=...

#+end_src

*** Dataset

The dataset used is Volcano and can be downloaded [[https://drive.google.com/file/d/1WgEOpawpyV_1lf80zpkz47VNtVqHR3ZK/view][here]]. Download it to your desired dataset directory and run:

#+begin_src bash
tar -xvf InSAR_Volcano.tar
#+end_src

After this, your =$DATA_DIR= should contain a folder called =subset_128= such that this following directory =$DATA_DIR/subset_128/= exists and has all the necessary files inside it.

For more details on the Volcano dataset, please consult the GANO paper:

#+begin_src
@article{rahman2022generative,
  title={Generative adversarial neural operators},
  author={Rahman, Md Ashiqur and Florez, Manuel A and Anandkumar, Anima and Ross, Zachary E and Azizzadenesheli, Kamyar},
  journal={arXiv preprint arXiv:2205.03017},
  year={2022}
}
#+end_src

** Running diffusion experiments

To run a diffusion experiment /locally/, simply cd into =exps= and run:

#+begin_src 
RUN_LOCAL=1 bash main.sh sbgm <experiment name> <json file>
#+end_src

This will run an experiment whose results directory will be created in =$SAVEDIR/<slurm job id>/<experiment name>= with hyperparameters selected from =<json file>=. The =RUN_LOCAL= command simply says that the code should be run from the parent directory of =exps=, rather than copy the code over to =$SAVEDIR/<id>/<experiment name>= and run from there (the latter should only be done when an actual experiment is being launched from Slurm).

If you are not running this on a Slurm environment, then you should also define =SLURM_JOB_ID= to something that can uniquely identify your experiment.

#+begin_src 
RUN_LOCAL=1 SLURM_JOB_ID=`date +%s` bash main.sh <experiment name> <json file>
#+end_src

In the results directory, the following things are saved periodically:

- =noise/noise_samples.{pdf,png}=: samples from the noise distribution. If you are using RBF noise (i.e. =white_noise=False=) then you can play around with =rbf_scale= and see how that affects the smoothness of the noise samples.
- =noise/init_samples.{pdf,png}=: ignore this, it should be the same as the above.
- =noise/noise_sampler_C.{pdf,png}=: the first 200 cols/rows of the computed covariance matrix.
- =u_noised.png=: for a random image (function) from the training set =u=, show the function =u + c * z=, where =c= is a coefficient from =σ_1= to =σ_L= and =z= is a sample from the noise distribution.
- =samples/<epoch>.{pdf,png}=: samples generated from the model after this particular epoch of training.

For Slurm-enabled clusters, simply run:

#+begin_src  bash
sbatch launch_exp.sh sbgm <experiment name> <json file>
#+end_src

Note that the SBATCH flags at the top of =launch_exp.sh= should be modified according to what is appropriate for your cluster environment (e.g. GPU selected, available memory, etc.).

** Running experiments

Experiments are run by running a launch script as well as specifying the path to a json file which details all of the hyperparameters to be used. To see what hyperparameters exist, please consult the `Arguments` dataclass in =train.py=.

For the following commands, you are not using Slurm, simply set SLURM_JOB_ID to something random and launch with =bash= instead of =sbatch=.

*** Baseline experiment (independent noise)

cd into =exps= and run:

#+begin_src bash
sbatch launch_exp.sh sbgm indep_experiment json/indep-copied.json
#+end_src

*** RBF experiment (structured noise)

cd into =exps= and run:

#+begin_src bash
sbatch launch_exp.sh sbgm rbf_experiment json/rbf-copied.json
#+end_src

Note that in =rbf-copied.json=, you can modify =rbf_scale= for different levels of smoothness. 

** Evaluation

We have a separate evaluation script which can be used to generate a larger set of samples, as well as a larger set of generated examples from which histograms for variance and skew can be computed.

Here is an example script which loads in the pretrained model corresponding to the best value of =w_total=, and generates 1024 samples with a batch size of 128. This will take a while.

#+begin_src bash
python eval.py --exp_name=${SAVEDIR}/${EXP_NAME} \
--Ntest=1024 \
--val_batch_size=128 \
--savedir="${SAVEDIR}/${EXP_NAME}/eval" \
--mode=generate \
--checkpoint="model.w_total.pt"
#+end_src

To generate plots corresponding to various files produced by this, simply replace mode=generate with mode=plot.

Some things to note:

- Computing the validation metrics takes a long time, even more so if =Ntest= is large. For our experiments, we use =256= which can still take a while, depending on what the validation batch size is used.

*** Baseline experiment

Download the pre-trained checkpoint [[https://drive.google.com/file/d/1zLFWZ3JOYAiDUkCJOFY1ma_Nqc56MOET/view?usp=drive_link][here]]. Download it to your predefined =$SAVEDIR= and run:

#+begin_src bash
tar -xvzf rbf-checkpoint.tar.gz
#+end_src

Then cd back into this repo into =exps= and run:

#+begin_src bash
bash launch_eval.py ...
#+end_src

*** RBF experiment

Download the pretrained checkpoint here.

#+begin_src bash
bash launch_eval.py ...
#+end_src

Here are some examples: ...

[[./assets/final_stats.png]]

[[./assets/rbf_samples_wtotal.png]]

** Future work

- One way in which quality of life can be significantly improved is to consider sampling algorithms for diffusion which are faster without sacrificing sample quality.
